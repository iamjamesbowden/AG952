{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b59641",
   "metadata": {},
   "source": "**L6 Pre-Processing Pipeline -- Lecture Demonstration**\n\nRun time: approximately 12 minutes. This notebook accompanies Section 6.4 of the Week 6 lecture. For the full pipeline implementation including N-grams, TF-IDF weighting, and workshop exercises, see L6\\_preprocessing\\_pipeline\\_WORKSHOP.ipynb."
  },
  {
   "cell_type": "code",
   "id": "57299b27",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import re, warnings\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mc\nimport numpy as np\nwarnings.filterwarnings('ignore')\n\n# -- NLTK --------------------------------------------------------------------\nimport nltk\nfrom nltk.stem import PorterStemmer\n\nfor _path, _pkg in [('tokenizers/punkt_tab', 'punkt_tab'),\n                    ('corpora/stopwords',    'stopwords')]:\n    try:\n        nltk.data.find(_path)\n    except LookupError:\n        try:    nltk.download(_pkg, quiet=True)\n        except Exception: pass\n\ntry:\n    from nltk.tokenize import word_tokenize as _wt\n    _NLTK_TOK = True\nexcept ImportError:\n    _NLTK_TOK = False\n\ntry:\n    from nltk.corpus import stopwords as _sw\n    _NLTK_SW = set(_sw.words('english'))\nexcept Exception:\n    _NLTK_SW = {\n        'i','me','my','we','our','you','your','he','him','his','she','her',\n        'it','its','they','them','their','what','which','who','this','that',\n        'these','those','am','is','are','was','were','be','been','being',\n        'have','has','had','do','does','did','a','an','the','and','but',\n        'if','or','as','of','at','by','for','with','about','into','through',\n        'during','before','after','to','from','in','out','on','off','then',\n        'here','there','when','where','how','all','both','each','more',\n        'other','some','than','so','also','any','now','per','re','just',\n        'very','still','back','even','well','way','where','much','above',\n    }\n\n# -- spaCy -------------------------------------------------------------------\ntry:\n    import spacy\n    _nlp = spacy.load('en_core_web_sm')\n    _SPACY = True\nexcept (ImportError, OSError):\n    _SPACY = False\n\ndef lemmatise(tokens):\n    if _SPACY:\n        return [t.lemma_.lower() for t in _nlp(' '.join(tokens))]\n    out = []\n    for w in tokens:\n        if   w.endswith('ies') and len(w) > 4: out.append(w[:-3] + 'y')\n        elif w.endswith('ing') and len(w) > 5: out.append(w[:-3])\n        elif w.endswith('ed')  and len(w) > 4: out.append(w[:-2])\n        elif w.endswith('es')  and len(w) > 3: out.append(w[:-1])\n        elif w.endswith('s')   and len(w) > 3 and not w.endswith('ss'): out.append(w[:-1])\n        else: out.append(w)\n    return out\n\n# -- Tokeniser ---------------------------------------------------------------\n_stemmer = PorterStemmer()\n\ndef tokenise(text):\n    lo = text.lower()\n    if _NLTK_TOK:\n        try:\n            toks = _wt(lo)\n        except Exception:\n            toks = re.findall(r'[a-z]+', lo)\n    else:\n        toks = re.findall(r'[a-z]+', lo)\n    return [t for t in toks if t.isalpha()]\n\n# -- Stop-word lists ---------------------------------------------------------\n# NLTK English list includes all six modals: will, may, could, should, would, must\nMODALS    = {'will', 'may', 'could', 'should', 'would', 'must'}\nNEGATIONS = {'not', 'no', 'nor', 'never', 'neither', 'without', 'cannot'}\nPROTECTED = MODALS | NEGATIONS\nSTD_SW    = _NLTK_SW\nFIN_SW    = _NLTK_SW - PROTECTED   # finance-adjusted: keeps modals and negation\n\n# -- Loughran-McDonald word lists (representative subset) --------------------\nLM_NEG = {\n    'loss','losses','decline','decrease','decreased','failure','fail','failed',\n    'risk','risks','uncertain','uncertainty','uncertainties','adverse','adversely',\n    'violation','impair','impairment','restate','restated','litigation','default',\n    'difficult','difficulties','inability','unable','weakness','weaknesses',\n    'deteriorate','deterioration','negative','negatively','harm','harmful',\n    'shortage','concern','concerns','volatile','volatility','severe',\n    'insufficient','challenge','challenges','constrain','constraint','constraints',\n    'persist','intervention','regulatory',\n}\nLM_POS = {\n    'achieve','achieved','achievement','growth','grew','increase','increased',\n    'increasing','improve','improved','improvement','strong','strength',\n    'excellent','outstanding','positive','positively','opportunity','opportunities',\n    'benefit','benefits','beneficial','confidence','confident','efficient',\n    'effective','success','successful','generate','generates','generating',\n    'elevated','substantially','significant','favorable','favourable','record',\n}\nLM_ALL   = LM_NEG | LM_POS\nLM_STEMS = {_stemmer.stem(w) for w in LM_ALL}\n\n# -- Dark theme --------------------------------------------------------------\nBG   = '#1a1a2e'\nAXES = '#0f172a'\nTEXT = '#e2e8f0'\nGRID = '#2d3748'\n\nplt.rcParams.update({\n    'figure.facecolor':   BG,\n    'axes.facecolor':     AXES,\n    'axes.edgecolor':     GRID,\n    'axes.labelcolor':    TEXT,\n    'xtick.color':        TEXT,\n    'ytick.color':        TEXT,\n    'text.color':         TEXT,\n    'axes.titlecolor':    TEXT,\n    'grid.color':         GRID,\n    'grid.linewidth':     0.5,\n    'axes.grid':          True,\n    'axes.grid.axis':     'x',\n    'font.family':        'sans-serif',\n    'font.size':          9,\n    'axes.titlesize':     10,\n    'axes.spines.top':    False,\n    'axes.spines.right':  False,\n    'axes.spines.left':   False,\n    'axes.spines.bottom': False,\n})\n\ndef _colours(n):\n    s, e = mc.hex2color('#7c3aed'), mc.hex2color('#1d4ed8')\n    return [mc.to_hex([s[j] + (e[j] - s[j]) * i / max(n - 1, 1) for j in range(3)])\n            for i in range(n)]\n\ndef hbar(ax, labels, values, title, xlabel='Count'):\n    n = len(labels)\n    ax.barh(range(n), values, color=_colours(n), height=0.7)\n    ax.set_yticks(range(n))\n    ax.set_yticklabels(labels, fontsize=8)\n    ax.invert_yaxis()\n    ax.set_xlabel(xlabel, fontsize=9)\n    ax.set_title(title, fontsize=10, fontweight='bold', pad=8)\n\nprint('Setup complete.')\n"
  },
  {
   "cell_type": "markdown",
   "id": "49fe7c83",
   "metadata": {},
   "source": "The excerpt below is drawn from the Enron email corpus compiled by Klimt and Yang (2004), released into the public domain by the Federal Energy Regulatory Commission during the 2001 investigation into Enron Corporation. It is used in this module because it combines the hedged, forward-looking register of corporate financial communication with the informal directness of internal correspondence, making it a productive corpus for demonstrating why generic preprocessing pipelines are ill-suited to financial text analysis."
  },
  {
   "cell_type": "code",
   "id": "65803403",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "EXCERPT = (\n    \"The California electricity markets continue to present significant opportunities \"\n    \"for our wholesale trading operations. We believe that power prices will remain \"\n    \"elevated through the first quarter and could increase further if supply constraints \"\n    \"persist. Our risk management team has concluded that we should maintain current \"\n    \"forward positions and not reduce our hedging exposure at this time.\\n\\n\"\n    \"We are not in a position to forecast exact price movements with certainty, nor can \"\n    \"we guarantee that our contractual obligations will be met under all market \"\n    \"scenarios. Management believes the company is well positioned to navigate these \"\n    \"uncertainties. Investors should be aware that forward-looking statements in this \"\n    \"communication may differ materially from actual results.\\n\\n\"\n    \"Revenue from gas and power trading operations increased substantially in the period \"\n    \"under review. This increase reflects both higher commodity prices and the improved \"\n    \"execution of our trading strategies. We must note that past performance should not \"\n    \"be taken as indicative of future results. Strong growth in operating earnings \"\n    \"reflects the underlying strength of our core business. We would encourage senior \"\n    \"management to consider these results in the context of the current regulatory \"\n    \"environment. The risk of further regulatory intervention remains significant, and \"\n    \"market participants could face challenges if conditions deteriorate.\"\n)\n\nprint(f'Corpus: {len(EXCERPT.split())} words.')\nprint()\nprint(EXCERPT)\n"
  },
  {
   "cell_type": "markdown",
   "id": "abac9cf1",
   "metadata": {},
   "source": "**Block 1: Raw text and unprocessed frequency counts.**"
  },
  {
   "cell_type": "code",
   "id": "4b20b922",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# No cleaning whatsoever: whitespace split only\nraw_tokens = EXCERPT.split()\nraw_freq   = Counter(raw_tokens).most_common(20)\nlabels, values = zip(*raw_freq)\n\nfig, ax = plt.subplots(figsize=(9, 5))\nhbar(ax, list(labels), list(values), 'Top 20 Raw Tokens -- no cleaning applied')\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "abf94668",
   "metadata": {},
   "source": "This is what a word frequency model sees before any pre-processing. Consider whether this frequency distribution tells you anything meaningful about the document."
  },
  {
   "cell_type": "markdown",
   "id": "600b1a3b",
   "metadata": {},
   "source": "**Block 2: Stop-word removal -- standard list versus finance-adjusted list.**\n\nNLTK's default English stop-word list includes all six modal verbs (will, may, could, should, would, must) and negation terms (not, no, nor). The finance-adjusted list removes these from the stop list, preserving them as tokens."
  },
  {
   "cell_type": "code",
   "id": "7351b584",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "tokens = tokenise(EXCERPT)\n\n# Standard stop-word removal\nstd_tokens = [t for t in tokens if t not in STD_SW and len(t) > 1]\nstd_freq   = Counter(std_tokens).most_common(20)\n\n# Finance-adjusted: protects modal verbs and negation\nfin_tokens = [t for t in tokens if t not in FIN_SW and len(t) > 1]\nfin_freq   = Counter(fin_tokens).most_common(20)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nhbar(ax1, [w for w, _ in std_freq], [c for _, c in std_freq],\n     'Standard Stop-Word Removal')\nhbar(ax2, [w for w, _ in fin_freq], [c for _, c in fin_freq],\n     'Finance-Adjusted (Modals + Negation Protected)')\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "91c15d22",
   "metadata": {},
   "source": "The left panel removes modal verbs. The right panel protects them. Which output would you run a sentiment analysis on?"
  },
  {
   "cell_type": "markdown",
   "id": "d74ad342",
   "metadata": {},
   "source": "**Block 3: Stemming versus lemmatisation, and the effect on dictionary matching.**"
  },
  {
   "cell_type": "code",
   "id": "f33c2ba1",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "tokens  = tokenise(EXCERPT)\ncontent = [t for t in tokens if t not in STD_SW and len(t) >= 3]\ntop20   = [w for w, _ in Counter(content).most_common(20)]\nstems   = [_stemmer.stem(w) for w in top20]\nlemmas  = lemmatise(top20)\n\n# -- Table (matplotlib, dark theme) -----------------------------------------\nfig_t, ax_t = plt.subplots(figsize=(9, 5))\nax_t.axis('off')\nfig_t.patch.set_facecolor(BG)\nax_t.set_facecolor(BG)\n\ntbl = ax_t.table(\n    cellText=list(zip(top20, stems, lemmas)),\n    colLabels=['Token', 'Porter Stem', 'SpaCy Lemma'],\n    loc='center', cellLoc='center'\n)\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(8.5)\ntbl.scale(1, 1.35)\nfor (row, col), cell in tbl.get_celld().items():\n    if row == 0:\n        cell.set_facecolor('#3730a3')\n        cell.set_text_props(color='white', fontweight='bold')\n    elif row % 2 == 0:\n        cell.set_facecolor(AXES)\n        cell.set_text_props(color=TEXT)\n    else:\n        cell.set_facecolor(BG)\n        cell.set_text_props(color=TEXT)\n    cell.set_edgecolor(GRID)\n\nax_t.set_title('Top 20 Content Words: Token vs Porter Stem vs SpaCy Lemma',\n               fontsize=10, fontweight='bold', color=TEXT, pad=12)\nplt.tight_layout()\nplt.show()\n\n# -- LM match rates ----------------------------------------------------------\nn             = len(top20)\nlemma_matches = sum(1 for l in lemmas if l in LM_ALL)\nstem_matches  = sum(1 for s in stems  if s in LM_STEMS)\n\nfig_b, ax_b = plt.subplots(figsize=(8, 2.5))\nfig_b.patch.set_facecolor(BG)\ncategories = ['Lemmatised', 'Stemmed (Porter)']\nrates      = [lemma_matches / n * 100, stem_matches / n * 100]\nax_b.barh(categories, rates, color=['#7c3aed', '#1d4ed8'], height=0.45)\nfor i, (v, m) in enumerate(zip(rates, [lemma_matches, stem_matches])):\n    ax_b.text(v + 0.5, i, f'{v:.0f}%  ({m}/{n})', va='center',\n              fontsize=9, color=TEXT)\nax_b.set_xlabel('LM Dictionary Match Rate (%)', fontsize=9)\nax_b.set_xlim(0, 60)\nax_b.set_title('Loughran-McDonald Match Rate: Lemmatised vs Stemmed',\n               fontsize=10, fontweight='bold', pad=8)\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "01cf362c",
   "metadata": {},
   "source": "The match rate difference is not a minor inconvenience. It is the difference between a model that measures sentiment and one that measures noise."
  },
  {
   "cell_type": "markdown",
   "id": "4f91a287",
   "metadata": {},
   "source": "**Block 4: Full recommended pipeline -- before and after.**"
  },
  {
   "cell_type": "code",
   "id": "e0bd4113",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Before: lowercase only, no other processing\nbefore_tokens = re.findall(r'[a-z]+', EXCERPT.lower())\nbefore_freq   = Counter(before_tokens).most_common(20)\n\n# Full pipeline: tokenise, finance-adjusted stop-word removal,\n# lemmatise, prune tokens shorter than 3 characters\npipeline_tokens = tokenise(EXCERPT)\npipeline_tokens = [t for t in pipeline_tokens if t not in FIN_SW]\npipeline_tokens = lemmatise(pipeline_tokens)\npipeline_tokens = [t for t in pipeline_tokens if len(t) >= 3 and t.isalpha()]\nafter_freq      = Counter(pipeline_tokens).most_common(20)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nhbar(ax1, [w for w, _ in before_freq], [c for _, c in before_freq],\n     'Before: Lowercase Only')\nhbar(ax2, [w for w, _ in after_freq],  [c for _, c in after_freq],\n     'After: Full Recommended Pipeline')\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "7a64cb0e",
   "metadata": {},
   "source": "**Block 5: The diagnostic cell -- a deliberately misconfigured pipeline.**"
  },
  {
   "cell_type": "code",
   "id": "85248da7",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Misconfigured pipeline:\n#   Porter stemmer applied\n#   Standard stop-word list (no modal verb or negation protection)\n#   No length pruning\ntokens      = tokenise(EXCERPT)\nbad_tokens  = [t for t in tokens if t not in STD_SW]\nbad_stemmed = [_stemmer.stem(t) for t in bad_tokens]\nbad_freq    = Counter(bad_stemmed).most_common(20)\n\nfig, ax = plt.subplots(figsize=(9, 5))\nhbar(ax, [w for w, _ in bad_freq], [c for _, c in bad_freq],\n     'Misconfigured Pipeline: Porter Stem + Standard Stop-Words + No Pruning')\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "141f2345",
   "metadata": {},
   "source": "Identify what is wrong with this pipeline and what the consequences would be for a Loughran-McDonald sentiment analysis."
  },
  {
   "cell_type": "markdown",
   "id": "a328e56f",
   "metadata": {},
   "source": "**References**\n\nGrimmer, J., Roberts, M.E. and Stewart, B.M. (2022) *Text as Data: A New Framework for Machine Learning and the Social Sciences*. Princeton: Princeton University Press.\n\nKlimt, B. and Yang, Y. (2004) 'The Enron Corpus: A New Dataset for Email Classification Research', in *Machine Learning: ECML 2004*, Lecture Notes in Computer Science, vol. 3201. Berlin: Springer, pp. 217--226. doi: 10.1007/978-3-540-30115-8\\_22.\n\nLoughran, T. and McDonald, B. (2011) 'When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks', *Journal of Finance*, 66(1), pp. 35--65. doi: 10.1111/j.1540-6261.2010.01625.x.\n\nManning, C.D. and Schutze, H. (1999) *Foundations of Statistical Natural Language Processing*. Cambridge, MA: MIT Press.\n\nPorter, M.F. (1980) 'An algorithm for suffix stripping', *Program*, 14(3), pp. 130--137. doi: 10.1108/eb046814."
  }
 ]
}