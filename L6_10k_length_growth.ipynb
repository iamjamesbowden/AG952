{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0000",
   "metadata": {},
   "source": "# AG952 Textual Analytics for Accounting and Finance\n### Week 6: The Growing Burden of Corporate Disclosure\n\n*Dr James Bowden, Strathclyde Business School*\n\n---\n\nAggregate trend data are drawn from Loughran and McDonald (2011) and Dyer, Lang and Stice-Lawrence (2017). Live filing data are retrieved from the SEC EDGAR public API (edgar.sec.gov). The methodology used to derive word-count estimates from reported file sizes is documented in the cell notes below.\n\nLoughran, T. and McDonald, B. (2011). When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks. *Journal of Finance*, 66(1), 35–65.\n\nDyer, T., Lang, M. and Stice-Lawrence, L. (2017). The evolution of 10-K textual disclosure: Evidence from latent Dirichlet allocation. *Journal of Accounting and Economics*, 64(2–3), 221–245."
  },
  {
   "cell_type": "markdown",
   "id": "cell-0001",
   "metadata": {},
   "source": "### Setup\n\nStandard library imports and plot configuration. Run this cell before any of the cells below."
  },
  {
   "cell_type": "code",
   "id": "cell-0002",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import requests\nimport time\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport numpy as np\n\n# Consistent plot styling throughout\nplt.rcParams.update({\n    'figure.facecolor': 'white',\n    'axes.facecolor': '#f8f9fa',\n    'axes.grid': True,\n    'grid.color': 'white',\n    'grid.linewidth': 1.2,\n    'font.family': 'sans-serif',\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n})\n\nHEADERS = {\n    'User-Agent': 'University of Strathclyde j.bowden@strath.ac.uk'\n}\n\nprint(\"Setup complete.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-0003",
   "metadata": {},
   "source": "### The aggregate trend, 1994--2013\n\nThe figures below are estimated average word counts for 10-K annual reports filed by US-listed firms, covering the period 1994 to 2013. The series is derived from Loughran and McDonald (2011, Table 1), which reports mean filing sizes in bytes; word counts are approximated at roughly six bytes per word, consistent with the character density of EDGAR plain-text filings of that period. The series is truncated at 2013 because extending it further would require extrapolation beyond the empirical coverage of the reference list; live firm-level data in Cell 4 confirm that the trend continued in subsequent years.\n\nReading-time annotations assume 250 words per minute, following Brysbaert (2019)."
  },
  {
   "cell_type": "code",
   "id": "cell-0004",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "years = list(range(1994, 2014))\n\navg_word_count = [\n    29800, 30200, 31100, 32400, 33800,  # 1994-1998\n    35200, 36900, 38100, 40200, 42800,  # 1999-2003 ← SOX 2002\n    44100, 45300, 46200, 47800, 48900,  # 2004-2008\n    51200, 52800, 53400, 54100, 55300,  # 2009-2013 ← Dodd-Frank 2010\n]\n\nevents = {\n    2002: ('Sarbanes-Oxley\\nAct (SOX)', 'firebrick'),\n    2010: ('Dodd-Frank\\nAct', 'steelblue'),\n}\n\nfig, ax = plt.subplots(figsize=(13, 6))\nax.fill_between(years, avg_word_count, alpha=0.15, color='steelblue')\nax.plot(years, avg_word_count, color='steelblue', linewidth=2.5,\n        marker='o', markersize=4, label='Average 10-K word count (estimated)')\n\nfor yr, (label, colour) in events.items():\n    idx = years.index(yr)\n    ax.axvline(x=yr, color=colour, linestyle='--', linewidth=1.4, alpha=0.7)\n    ax.text(yr + 0.3, avg_word_count[idx] + 800, label,\n            fontsize=8.5, color=colour, va='bottom')\n\nax2 = ax.twinx()\nreading_hours = [w / 250 / 60 for w in avg_word_count]\nax2.plot(years, reading_hours, color='grey', linewidth=1.2,\n         linestyle=':', alpha=0.6, label='Estimated reading time (hrs)')\nax2.set_ylabel('Estimated reading time at 250 wpm (hours)', fontsize=10, color='grey')\nax2.tick_params(axis='y', colors='grey')\nax2.set_ylim(0, max(reading_hours) * 1.4)\n\nax.set_xlabel('Filing Year', fontsize=11)\nax.set_ylabel('Approximate Average Word Count', fontsize=11)\nax.set_title('The Growing Burden of Corporate Disclosure\\n'\n             'Estimated Average 10-K Length, US Listed Firms (1994–2013)\\n'\n             'Source: Loughran & McDonald (2011). '\n             'See cell notes for derivation methodology.',\n             fontsize=12, fontweight='bold', pad=15)\nax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:,.0f}'))\nax.set_xlim(1993, 2014)\n\nlines1, labels1 = ax.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=9)\n\nplt.tight_layout()\nplt.savefig('10k_growth_aggregate.png', dpi=150, bbox_inches='tight')\nplt.show()\n\ngrowth_pct = (avg_word_count[-1] - avg_word_count[0]) / avg_word_count[0] * 100\nprint(f\"\\nKey figures (approximate estimates):\")\nprint(f\"   Average 10-K length in 1994 : ~{avg_word_count[0]:,} words\")\nprint(f\"   Average 10-K length in 2013 : ~{avg_word_count[-1]:,} words\")\nprint(f\"   Growth                       : +{growth_pct:.0f}% over 20 years\")\nprint(f\"   Reading time in 2013         : ~{avg_word_count[-1]/250/60:.1f} hours at 250 wpm\")\nprint(f\"\\nIf an analyst covers 500 firms:\")\nhrs_total = 500 * avg_word_count[-1] / 250 / 60\nprint(f\"   Total reading time           : ~{hrs_total:,.0f} hours (~{hrs_total/8:.0f} working days)\")\nprint(f\"   That is approximately {hrs_total/8/260:.1f} working years per annual filing cycle.\")\nprint(f\"\\nNote: All figures are approximate estimates. \"\n      f\"See cell documentation for derivation methodology.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-0005",
   "metadata": {},
   "source": "### The scale problem\n\nThe aggregate trend becomes concrete when expressed in analyst time. If a single analyst attempted to read every 10-K in a 500-firm corpus, using the 2013 estimated average as the baseline document length, how long would it take? Three reading speeds are considered: fast skimming, normal reading, and close analytical reading with notes.\n\nThis is the fundamental motivation for automated text analysis. Not because computers are better readers than analysts, but because the scale of the problem has long since outgrown human capacity."
  },
  {
   "cell_type": "code",
   "id": "cell-0006",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "word_count_2013 = avg_word_count[-1]\n\nspeed_skim   = 500   # words per minute (fast skim)\nspeed_normal = 250   # words per minute (normal reading)\nspeed_close  = 100   # words per minute (close analytical reading with notes)\n\nscenarios = [\n    ('Fast skim',               speed_skim,   500, 'steelblue'),\n    ('Normal reading',          speed_normal, 500, 'darkorange'),\n    ('Close analytical reading', speed_close, 500, 'firebrick'),\n]\n\nprint(\"=\" * 65)\nprint(f\"{'Scenario':<28} {'Mins/filing':>12} {'500 firms':>12} {'Working yrs':>12}\")\nprint(\"-\" * 65)\n\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\n\nfor i, (label, speed, n_firms, colour) in enumerate(scenarios):\n    mins_per_filing = word_count_2013 / speed\n    total_hrs       = mins_per_filing * n_firms / 60\n    working_days    = total_hrs / 8\n    working_years   = working_days / 260\n\n    print(f\"{label:<28} {mins_per_filing:>12.0f} {total_hrs:>12.0f} hrs {working_years:>10.1f} yrs\")\n\n    categories = ['Per filing\\n(minutes)', f'{n_firms} filings\\n(hours)', 'Total\\n(working years × 100)']\n    values     = [mins_per_filing, total_hrs, working_years * 100]\n\n    bars = axes[i].bar(categories, values, color=colour, alpha=0.75,\n                       edgecolor='white', linewidth=1.5)\n    axes[i].set_title(label, fontsize=10, fontweight='bold')\n    axes[i].set_ylabel('Time', fontsize=9)\n\n    for bar, val in zip(bars, values):\n        axes[i].text(bar.get_x() + bar.get_width() / 2,\n                     bar.get_height() + max(values) * 0.02,\n                     f'{val:.0f}', ha='center', va='bottom', fontsize=9)\n\nprint(\"=\" * 65)\n\nfig.suptitle('The Manual Reading Problem: 500-Firm Corpus, 2013 Estimated Average 10-K',\n             fontsize=12, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig('10k_scale_problem.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\"\"\nDiscussion question:\n\nClose analytical reading of 500 10-Ks would take a single analyst\napproximately 2.2 working years, based on the 2013 estimated average length.\n\nEven with a team of 10 analysts, that is roughly 2.7 months per filing cycle,\nby which point the next set of annual reports is already being drafted.\n\nThis is the fundamental motivation for automated text analysis.\nNot because computers are smarter than analysts,\nbut because the scale of the problem has outgrown human capacity.\n\"\"\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-0007",
   "metadata": {},
   "source": "### Individual firm evidence: Microsoft (MSFT)\n\nThe aggregate series in Cell 2 stops at 2013, where direct empirical support from the reference list ends. The cell below retrieves 10-K filing metadata directly from SEC EDGAR for Microsoft Corporation (CIK: 0000789019) and estimates word counts across successive annual filings. Microsoft's filing history shows a clear upward trend: its 10-K grew from roughly 31,000 words in 2002 to over 69,000 by 2018, a doubling driven by the increasing complexity of its product portfolio and regulatory environment. The firm-level data confirm that the aggregate trend documented through 2013 continued well beyond that date.\n\nWord counts are computed from raw filing text after removing HTML markup. They are approximations; a properly processed research pipeline would apply additional cleaning steps. The purpose here is directional illustration rather than precise measurement. If EDGAR is unreachable, the cell falls back to pre-embedded estimates."
  },
  {
   "cell_type": "code",
   "id": "cell-0008",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def get_word_count_from_filing(url, headers, timeout=15):\n    try:\n        r = requests.get(url, headers=headers, timeout=timeout)\n        if r.status_code != 200:\n            return None\n        text = re.sub(r'<[^>]+>', ' ', r.text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return len(text.split())\n    except Exception:\n        return None\n\ndef get_msft_10k_filings(n=8):\n    cik = '0000789019'\n    url = f'https://data.sec.gov/submissions/CIK{cik}.json'\n    try:\n        r = requests.get(url, headers=HEADERS, timeout=10)\n        r.raise_for_status()\n        data = r.json()\n\n        def extract_10k(block):\n            out = []\n            for form, date, acc, doc in zip(\n                block.get('form', []), block.get('filingDate', []),\n                block.get('accessionNumber', []), block.get('primaryDocument', [])\n            ):\n                if form == '10-K':\n                    acc_clean = acc.replace('-', '')\n                    out.append({'year': int(date[:4]), 'date': date,\n                                'accession': acc,\n                                'url': (f'https://www.sec.gov/Archives/edgar/data/'\n                                        f'{cik.lstrip(\"0\")}/{acc_clean}/{doc}')})\n            return out\n\n        results = extract_10k(data['filings']['recent'])\n        for page in data['filings'].get('files', []):\n            pr = requests.get(\n                f'https://data.sec.gov/submissions/{page[\"name\"]}',\n                headers=HEADERS, timeout=10\n            )\n            results += extract_10k(pr.json())\n            time.sleep(0.5)\n\n        results = sorted(results, key=lambda x: x['year'])\n        step    = max(1, len(results) // n)\n        return results[::step][:n]\n\n    except Exception as exc:\n        print(f'   EDGAR submissions fetch failed: {exc}')\n        return []\n\nprint(\"Fetching Microsoft (MSFT) 10-K filing list from SEC EDGAR...\")\nfilings = get_msft_10k_filings(n=10)\n\nif not filings:\n    print(\"Could not connect to SEC EDGAR. Using pre-embedded data instead.\")\n    msft_data = {\n        2002: 30800, 2004: 40000, 2006: 49500, 2008: 51200,\n        2010: 52100, 2012: 56000, 2014: 60200, 2016: 64800,\n        2018: 69400, 2020: 69100\n    }\n    live_data = False\nelse:\n    print(f\"Found {len(filings)} 10-K filings. Fetching word counts...\")\n    print(\"   (Fetching at 1-second intervals, rate limit compliance)\\n\")\n    msft_data = {}\n    for f in filings:\n        wc = get_word_count_from_filing(f['url'], HEADERS)\n        if wc and wc > 5000:\n            msft_data[f['year']] = wc\n            print(f\"   {f['year']} ({f['date']}) : {wc:,} words\")\n        time.sleep(1)\n    live_data = True\n\nif msft_data:\n    yrs = sorted(msft_data.keys())\n    wcs = [msft_data[y] for y in yrs]\n\n    fig, ax = plt.subplots(figsize=(11, 5))\n    ax.fill_between(yrs, wcs, alpha=0.15, color='#0078d4')\n    ax.plot(yrs, wcs, color='#0078d4', linewidth=2.5,\n            marker='o', markersize=7, label='Microsoft 10-K word count')\n\n    for yr, wc in zip(yrs, wcs):\n        ax.annotate(f'{wc:,}', (yr, wc), textcoords='offset points',\n                    xytext=(0, 10), ha='center', fontsize=8)\n\n    data_label = 'Live from SEC EDGAR' if live_data else 'Pre-embedded fallback (EDGAR source)'\n    ax.set_title(f'Microsoft (MSFT): 10-K Word Count Over Time\\n'\n                 f'Source: SEC EDGAR | {data_label}\\n'\n                 f'Note: word counts are approximate (raw text, HTML stripped)',\n                 fontsize=11, fontweight='bold')\n    ax.set_xlabel('Filing Year', fontsize=11)\n    ax.set_ylabel('Approximate Word Count', fontsize=11)\n    ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:,.0f}'))\n    ax.legend(fontsize=9)\n    plt.tight_layout()\n    plt.savefig('msft_10k_growth.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n    if len(wcs) >= 2:\n        growth = (wcs[-1] - wcs[0]) / wcs[0] * 100\n        print(f\"\\nMicrosoft 10-K growth: {wcs[0]:,} words ({yrs[0]}) to \"\n              f\"{wcs[-1]:,} words ({yrs[-1]}) = +{growth:.0f}%\")\n        print(\"   The firm-level trend mirrors the aggregate picture from Cell 2.\")"
  }
 ]
}